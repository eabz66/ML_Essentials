INITIAL DATA EXPLORATION
========================
df = pd.read_csv("filename.csv")
df = pd.read_csv("https://raw.githubusercontent.com/fenago/pythonml/main/data/WA_Fn-UseC_-Telco-Customer-Churn.csv
df = pd.read_excel("filename.xls")
df.head()
df.tail()
df.sample(10)
df.shape
df.info()
df.describe(include='all').T
df.nunique()
df[col].unique()

df.isna().sum()
df.isnull().sum()

type(x)


DATA PREPARATION
================

df.columns = df.columns.str.lower().str.replace(' ', '_')
string_columns = list(df.dtypes[df.dtypes == 'object'].index)
for col in string_columns:
    df[col] = df[col].str.lower().str.replace(' ', '_')
    

- Handling duplicates
*********************
# Check for duplicates
df.duplicated().sum()

# Drop duplicate rows and keep the first occurrence
df.drop_duplicates(keep='first', inplace=True)

# Drop duplicate rows based on 'Name' and 'Age' columns
df.drop_duplicates(subset=['Name', 'Age'], keep='first', inplace=True)

- Converting data types
***********************
df.dtypes

df["numCol"] = df["numCol"].astype(float)
df["catCol"] = df["catCol"].astype("category")

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'] = df['TotalCharges'].fillna(0)

- Handling missing values
*************************
# Identifying missing values
df.isna().sum()
df.isnull().sum()

# Showing rows not having null values in the aCol column
df.notnull()
df[ df['aCol'].notnull() ]

# Removing rows with missing values
df = df.dropna(axis=0)
# Removing columns with missing values
df = df.dropna(axis=0)

# Filling missing values
df['numCol'].fillna(df['numCol'].mean(), inplace=True)
df['numCol'].fillna(df['numCol'].median(), inplace=True)
df['numCol'].fillna(df['numCol'].mode()[0], inplace=True)
df['catCol'].fillna("newVal", inplace=True)
df[c] = df[c].replace(to_replace=99999999, value=np.nan)

# Visualizing missing data
import seaborn as sns
sns.heatmap(df.isna(), cmap='viridis')
sns.heatmap(df.transpose().isnull(), cmap='viridis')

# Remove rows with specified threshold of missing values
df.dropna(thresh=int(df.shape[1] * (1 - threshold)), axis=0, inplace=True)
# Remove columns with specified threshold of missing values
df.dropna(thresh=int(df.shape[0] * (1 - threshold)), axis=1, inplace=True)

- Feature selection
*******************
# Drop unused columns
df = df.drop(["col1", "col2"], axis=1)

# View correlation matrix as a heatmap
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(20, 20))
sns.heatmap(df.corr(), cmap='coolwarm', annot=True, square=True)
plt.show()

# Remove any features that have a variance below a certain threshold
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
selector.fit(df)
# Get the indices of the selected features
selected_features = df.columns[selector.get_support(indices=True)]
# Remove the unselected features from the dataset
df_selected = df[selected_features]

# Address Multicollinearity
import statsmodels.api as sm
# Add a constant term to the dataset
df_selected = sm.add_constant(df_selected)
# Calculate the VIF for each feature
vif = pd.DataFrame()
vif["feature"] = df_selected.columns
vif["VIF"] = [variance_inflation_factor(df_selected.values, i) for i in range(df_selected.shape[1])]
print(vif)
# Features with a high VIF should be removed to address multicollinearity
df = df_selected.drop(['column1', 'column2', ...], axis=1)

- Data cleaning
***************
df.info()
df.info(verbose=True)
df.info(memory_usage='deep')
df.info(show_counts=True)

df.nunique()
df['aCol'].unique()

df['aCol'].value_counts()
df['aCol'].value_counts(normalize=True)
df[['col1', 'col2', 'col3']].value_counts()

# Filtering rows based on condition
df = df[ df['aCol'] == 'aValue' ]
df = df.query("aCol == 'aValue'")

# Change column names
df.rename(columns={'old_name1': 'new_name1', 'old_name2': 'new_name2'}, inplace=True)
df.columns = df.columns.str.replace('_pct', '_percent')
df.columns = df.columns.str.replace('^a_', 'annual_', regex=True)

# Drop columns
df = df.drop(columns=['col1', 'col2'])

# Split dataset into numeric and categorical variables
numerics = ['int16','int32','int64','float64']
catDF = df.select_dtypes(exclude=numerics)
numDF = df.select_dtypes(include=numerics)
catDF.head()
numDF.head()


UNIVARIATE ANALYSIS
===================
# histograms for all variables in dataset
df.hist(bins=25, figsize=(15, 25), layout=(-1, 5), edgecolor="black")
plt.tight_layout();


Numerical Variables
*******************
# Pairplot for all numerical variables
sns.pairplot(numDF, height=1.5, plot_kws={"s": 2, "alpha": 0.2});
plt.show()

# Counts as a bar plot
sns.countplot(x=colName, data=df)
plt.show()

# Histograms
plt.hist(df['age'], bins=20)
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

# Box-plots
plt.boxplot(df['age'])
plt.ylabel('Age')
plt.show()

# Density plots
df['age'].plot.kde()
plt.xlabel('Age')
plt.show()


summary_table = pd.DataFrame(columns=['Variable', 'Min', 'Max', 'Mean', 'Median', 'Std Dev'])
for col in df.columns:
	if df[col].dtypes != "object":
    	summary_table = summary_table.append({
        	'Variable': col,
        	'Min': df[col].min(),
        	'Max': df[col].max(),
        	'Mean': df[col].mean(),
        	'Median': df[col].median(),
        	'Std Dev': df[col].std()
    	}, ignore_index=True)
print(summary_table)


Categorical variables
*********************
print(colName + ' ' + str(sorted(df[colName].unique())))

df[colName].value_counts(dropna=False)	# Totals per value

df[colName].value_counts(dropna=False, normalize=True) # Percentages

counts = df['product_type'].value_counts()
counts.plot(kind='barh')  # or 'bar' and swap x/y labels
plt.xlabel('Count')
plt.ylabel(aCol)
plt.show()

freq_table = pd.crosstab(index=df['product_type'], columns='count')
freq_table['percentage'] = freq_table['count'] / len(df) * 100
print(freq_table)


df['product_type'].value_counts().plot(kind='density')
plt.xlabel('Product Type')
plt.show()



BIVARIATE ANALYSIS
==================
# Correlation plot
df_corr = df.corr(method="spearman") # pearson assumes a linear relationship... spearman does not
# Create labels for the correlation matrix
labels = np.where(np.abs(df_corr)>0.75, "S",
                  np.where(np.abs(df_corr)>0.5, "M",
                           np.where(np.abs(df_corr)>0.25, "W", "")))
# Plot correlation matrix
plt.figure(figsize=(15, 15))
sns.heatmap(df_corr, mask=np.eye(len(df_corr)), square=True,
            center=0, annot=labels, fmt='', linewidths=.5,
            cmap="vlag", cbar_kws={"shrink": 0.8});

N2N
***
# Scatter plot
plt.scatter(df['age'], df['income'])
plt.xlabel('Age')
plt.ylabel('Income')
plt.show()

# Calculate correlation coefficient
corr = df['Numeric Variable 1'].corr(df['Numeric Variable 2'], method='spearman')
print('Correlation Coefficient:', corr)

# Create heatmap
sns.heatmap(df.corr(method='spearman'))
plt.show()

# Regression line
import numpy as np
x = df['age']
y = df['income']
p = np.polyfit(x, y, 1)
line = np.poly1d(p)

plt.scatter(x, y)
plt.plot(x, line(x), color='red')
plt.xlabel('Age')
plt.ylabel('Income')
plt.show()


C2C
***
# Create frequency table
freq_table = df['Category 1'].value_counts()
print(freq_table)

# Create crosstab
crosstab = pd.crosstab(index=df['Category 1'], columns=df['Category 2'])
print(crosstab)

# Create stacked bar chart
df.groupby(['Category 1', 'Category 2'])['Count'].sum().unstack().plot(kind='bar', stacked=True)
plt.show()


from scipy.stats import chi2_contingency
# Create contingency table
crosstab = pd.crosstab(index=df['Category 1'], columns=df['Category 2'])

# Perform chi-square test
chi2, p, dof, expected = chi2_contingency(crosstab)
print('Chi-Square Statistic:', chi2)
print('P-Value:', p)


C2N
***
# Create box plot
df.boxplot(column='Numeric Variable', by='Categorical Variable')
plt.show()

# Create violin plot
sns.violinplot(x='Categorical Variable', y='Numeric Variable', data=df)
plt.show()

from scipy.stats import ttest_ind

# Create subsets of numeric variable based on categorical variable
subset1 = df.loc[df['Categorical Variable'] == 'Category 1']['Numeric Variable']
subset2 = df.loc[df['Categorical Variable'] == 'Category 2']['Numeric Variable']

# Perform t-test
t, p = ttest_ind(subset1, subset2)
print('T-Statistic:', t)
print('P-Value:', p)



MULTIVARIATE ANALYSIS
=====================
CCC
***
# Create heatmap
sns.heatmap(pd.crosstab(df['Gender'], [df['Education'], df['Credit_Status']]), annot=True, fmt="d", cmap="YlGnBu")
plt.show()

CCN
***
# Create grouped box plot
df.boxplot(column='Income', by=['Gender', 'Education'], figsize=(12,8))
plt.show()

NNC
***
# Create scatter plot with color-coded markers
sns.scatterplot(x="Age", y="Income", hue="Gender", data=df)
plt.show()

NNN
***
# Create pairplot
sns.pairplot(df, vars=['Age', 'Income', 'Credit_Score'], hue="Gender")
plt.show()


Analyzing Numeric Variables
***************************
# Create correlation matrix
corr = df.corr(method='spearman')
sns.heatmap(corr, annot=True, cmap="YlGnBu")
plt.show()


# Create scatter plot matrix
sns.pairplot(df, vars=['Age', 'Income', 'Credit_Score'], hue="Gender", markers=['o', 's'])
plt.show()



FEATURE ENGINEERING
===================
- Preprocess data
- Feature Selection
- Data Transformations
- Rescale Data
- Univariate Selection
- Recursive Feature Elimination
- Principle Component Analysis
- Feature Importance
- Algorithm Evaluation Metrics


MACHINE LEARNING
================
- Resampling methods
- Split into Train and Test sets
- K-fold Cross-Validation
- Leave One Out Cross-Validation
- Stratified K-Fold Cross-Validation
- What Technique to Use When
- Algorithm Evaluation Metrics
- Regression Metrics
- Classification Metrics


1) Structural Analysis: Embark on your journey by importing the necessary libraries, 
loading the dataset, and delving into its structure. Acquaint yourself with the data 
types, dimensions, and variables, laying a solid foundation for the subsequent stages. 
It’s crucial to understand the dataset and consult the data dictionary for context. 
Moral: Building a clear understanding of the data structure helps you make informed 
decisions throughout the data science process.

2) Quality Investigation: Identify duplicate records, null values, missing data, and 
outliers, then address these issues to ensure data quality. A clean dataset paves the way 
for accurate insights and reliable models. Moral: High-quality data forms the backbone 
of successful data science projects, as it directly impacts the validity of your results.

3) Simple Content Investigation: Explore distributions, patterns, and feature 
relationships within your dataset. This preliminary analysis helps you identify 
underlying trends and potential issues that warrant further examination. Moral: A basic 
content investigation sets the stage for deeper analysis, allowing you to develop 
hypotheses and refine your focus.

4) Complete Exploratory Data Analysis (EDA): Conduct univariate, bivariate, and 
multivariate analyses to uncover hidden patterns and relationships in the data. Document 
your insights meticulously, as this is one of the most crucial steps in the process. 
Moral: Comprehensive EDA allows you to thoroughly understand the data, generate valuable 
insights, and identify areas for further investigation or feature engineering.

5) Feature Selection with Data: Address missing values and remove unused columns. 
Eliminate uncorrelated, low-variance features, and tackle multicollinearity to refine 
your dataset for optimal model performance. Moral: Effective feature selection reduces 
noise, improves model interpretability, and helps prevent overfitting.

6) Feature Selection with Models: Utilize model-based techniques to identify significant 
features by examining coefficients, p-values, Variance Inflation Factor (VIF), Principal 
Component Analysis (PCA), and Recursive Feature Elimination (RFE). Moral: Model-driven 
feature selection provides an additional layer of scrutiny, ensuring that the most 
informative features are retained for modeling.

7) Normalize, Standardize, and Encode: Preprocess your data by normalizing or 
standardizing numerical variables, encoding categorical variables, and handling any 
resampling required for balanced datasets. Moral: Proper data preprocessing ensures that 
your models can learn effectively from the data and produce accurate, unbiased 
predictions.

8) Algorithm Harness: Run your data through multiple machine learning and deep learning 
algorithms to obtain baseline performance metrics. Employ k-fold cross-validation for 
robust performance evaluation and model selection. Moral: Testing a variety of algorithms 
helps you identify the best-performing models for your specific problem, allowing you to 
make informed decisions about which models to optimize and deploy.

9) Model Optimization: Identify the top 2–3 models based on their performance, and 
fine-tune them using hyperparameter tuning to achieve the best possible results. Moral: 
Model optimization ensures that you extract the maximum predictive power from your chosen 
models, leading to more accurate and reliable insights.

10) Model Deployment and UI: Deploy your chosen model, preferably as a microservice with 
an API. Finally, build a user-friendly interface to interact with your model seamlessly. 
Moral: Effective model deployment and a user-friendly interface enable stakeholders to 
access and utilize your model’s insights easily, maximizing the value and impact of your 
data science work.